{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook will take the cnn_wide.csv dataset and create a similar one that makes it take into account the previous second.  240xN --> 480xN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "import script as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116090, 243)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = pd.read_csv(r\"C:\\Users\\yashm\\Google Drive\\Data Capstone_\\Project Folder\\PreWideData\\cnn_wide.csv\")\n",
    "data = pd.read_csv(r'C:/Users/Hans/Documents/CalPoly4thYear/Data451/cnn_wide.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>232</th>\n",
       "      <th>233</th>\n",
       "      <th>234</th>\n",
       "      <th>235</th>\n",
       "      <th>236</th>\n",
       "      <th>237</th>\n",
       "      <th>238</th>\n",
       "      <th>239</th>\n",
       "      <th>labels</th>\n",
       "      <th>activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.605</td>\n",
       "      <td>0.617</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.621</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.605</td>\n",
       "      <td>0.590</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.934</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.609</td>\n",
       "      <td>0.918</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>-0.602</td>\n",
       "      <td>0.891</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.512</td>\n",
       "      <td>0.914</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>0.918</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.391</td>\n",
       "      <td>0.914</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>...</td>\n",
       "      <td>0.723</td>\n",
       "      <td>0.613</td>\n",
       "      <td>-0.680</td>\n",
       "      <td>0.727</td>\n",
       "      <td>0.598</td>\n",
       "      <td>-0.680</td>\n",
       "      <td>0.719</td>\n",
       "      <td>0.586</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.672</td>\n",
       "      <td>0.711</td>\n",
       "      <td>0.586</td>\n",
       "      <td>-0.648</td>\n",
       "      <td>0.684</td>\n",
       "      <td>0.582</td>\n",
       "      <td>-0.621</td>\n",
       "      <td>0.668</td>\n",
       "      <td>0.574</td>\n",
       "      <td>...</td>\n",
       "      <td>0.359</td>\n",
       "      <td>0.473</td>\n",
       "      <td>-0.867</td>\n",
       "      <td>0.359</td>\n",
       "      <td>0.434</td>\n",
       "      <td>-0.895</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.898</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.426</td>\n",
       "      <td>-0.879</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.441</td>\n",
       "      <td>-0.859</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.445</td>\n",
       "      <td>...</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.465</td>\n",
       "      <td>-0.875</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.445</td>\n",
       "      <td>-0.859</td>\n",
       "      <td>0.391</td>\n",
       "      <td>0.441</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.848</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.441</td>\n",
       "      <td>-0.848</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.438</td>\n",
       "      <td>-0.848</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.426</td>\n",
       "      <td>...</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.418</td>\n",
       "      <td>-0.801</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.422</td>\n",
       "      <td>-0.801</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 243 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   time      0      1      2      3      4      5      6      7      8  \\\n",
       "0   0.0 -0.605  0.617  0.070 -0.621  0.602  0.023 -0.605  0.590 -0.016   \n",
       "1   1.0 -0.512  0.914 -0.191 -0.441  0.918 -0.152 -0.391  0.914 -0.102   \n",
       "2   2.0 -0.672  0.711  0.586 -0.648  0.684  0.582 -0.621  0.668  0.574   \n",
       "3   3.0 -0.898  0.379  0.426 -0.879  0.387  0.441 -0.859  0.402  0.445   \n",
       "4   4.0 -0.848  0.395  0.441 -0.848  0.379  0.438 -0.848  0.367  0.426   \n",
       "\n",
       "     ...       232    233    234    235    236    237    238    239  labels  \\\n",
       "0    ...     0.934 -0.133 -0.609  0.918 -0.191 -0.602  0.891 -0.195       0   \n",
       "1    ...     0.723  0.613 -0.680  0.727  0.598 -0.680  0.719  0.586       0   \n",
       "2    ...     0.359  0.473 -0.867  0.359  0.434 -0.895  0.363  0.422       0   \n",
       "3    ...     0.402  0.465 -0.875  0.398  0.445 -0.859  0.391  0.441       0   \n",
       "4    ...     0.496  0.418 -0.801  0.496  0.422 -0.801  0.492  0.422       0   \n",
       "\n",
       "   activity  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "\n",
       "[5 rows x 243 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## transform into 160x3 columns (create lag)\n",
    "data[np.arange(240,480).astype(str)] = data[np.arange(240).astype(str)].shift(-1)\n",
    "data[['time','activity','labels']] = data[['time','activity','labels']].shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    64311\n",
       "3.0    22966\n",
       "2.0    13337\n",
       "1.0     8557\n",
       "6.0     5651\n",
       "4.0      840\n",
       "5.0      427\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for c in data.columns:\n",
    "#     print (c)\n",
    "data['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(data.index[len(data) - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = data.sample(int(len(data)*.7))\n",
    "test = data.drop(train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
       "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
       "       247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
       "       260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
       "       273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
       "       286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
       "       299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
       "       312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n",
       "       325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n",
       "       338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
       "       351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
       "       364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376,\n",
       "       377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389,\n",
       "       390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402,\n",
       "       403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415,\n",
       "       416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428,\n",
       "       429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441,\n",
       "       442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454,\n",
       "       455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,\n",
       "       468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(480)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train['activity']\n",
    "K = len(train['activity'].unique())\n",
    "features = train[np.arange(480).astype(str)]\n",
    "# features = train[train.columns[1:-2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((81262, 480), 81262, 2)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape, len(labels), K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.81599998  0.93400002 -0.83200002]\n",
      "  [-0.85500002  0.92199999 -0.87099999]\n",
      "  [-0.73000002  0.852      -0.87099999]\n",
      "  ..., \n",
      "  [-0.51999998  0.83999997  0.125     ]\n",
      "  [-0.54699999  0.86299998  0.168     ]\n",
      "  [-0.60500002  0.88300002  0.17200001]]\n",
      "\n",
      " [[-0.41800001  0.215      -0.852     ]\n",
      "  [ 0.19499999  0.37099999 -0.71100003]\n",
      "  [ 0.76599997  0.50400001 -0.699     ]\n",
      "  ..., \n",
      "  [ 0.50800002  0.086       0.67199999]\n",
      "  [ 0.42199999  0.09        0.85500002]\n",
      "  [ 0.414       0.039       1.01199996]]\n",
      "\n",
      " [[ 0.60500002  0.523      -0.64099997]\n",
      "  [ 0.58200002  0.53500003 -0.64099997]\n",
      "  [ 0.56300002  0.54699999 -0.64499998]\n",
      "  ..., \n",
      "  [ 0.52700001  0.574      -0.65200001]\n",
      "  [ 0.50800002  0.57800001 -0.65200001]\n",
      "  [ 0.5         0.57800001 -0.66000003]]\n",
      "\n",
      " ..., \n",
      " [[ 0.125       0.98000002  0.117     ]\n",
      "  [ 0.125       0.949       0.105     ]\n",
      "  [ 0.125       0.926       0.082     ]\n",
      "  ..., \n",
      "  [ 0.223       0.96499997  0.102     ]\n",
      "  [ 0.215       0.96899998  0.102     ]\n",
      "  [ 0.215       0.96499997  0.098     ]]\n",
      "\n",
      " [[-0.85900003 -0.5        -0.391     ]\n",
      "  [-0.801      -0.50800002 -0.34799999]\n",
      "  [-0.78100002 -0.523      -0.32800001]\n",
      "  ..., \n",
      "  [-0.78100002 -0.27000001 -0.63700002]\n",
      "  [-0.73799998 -0.30500001 -0.62900001]\n",
      "  [-0.71499997 -0.34       -0.62099999]]\n",
      "\n",
      " [[ 0.875       0.41800001 -0.094     ]\n",
      "  [ 0.82800001  0.43000001 -0.12899999]\n",
      "  [ 0.80900002  0.44499999 -0.125     ]\n",
      "  ..., \n",
      "  [ 0.801       0.465      -0.117     ]\n",
      "  [ 0.80900002  0.461      -0.105     ]\n",
      "  [ 0.852       0.43399999 -0.059     ]]]\n"
     ]
    }
   ],
   "source": [
    "# This is a sanity check to make sure TensorFlow is reshaping the features \n",
    "# into the 3 channels (x, y, z) correctly.\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    train_features = tf.placeholder(tf.float32, shape=(None, 480))\n",
    "    x = tf.reshape(train_features, [-1, 160, 3])\n",
    "    \n",
    "with tf.Session(graph=graph) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(sess.run(x, feed_dict={train_features: features}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Acc:  59.8151683807  Change:  59.8151683807\n",
      "Epoch:  1  Acc:  66.3754284382  Change:  6.56026005745\n",
      "Epoch:  2  Acc:  67.0325636864  Change:  0.657135248184\n",
      "Epoch:  3  Acc:  67.973959446  Change:  0.941395759583\n",
      "Epoch:  4  Acc:  69.256234169  Change:  1.28227472305\n",
      "Epoch:  5  Acc:  70.4732835293  Change:  1.21704936028\n",
      "Epoch:  6  Acc:  71.3691532612  Change:  0.895869731903\n",
      "Epoch:  7  Acc:  71.8909204006  Change:  0.521767139435\n",
      "Epoch:  8  Acc:  72.3597764969  Change:  0.468856096268\n",
      "Epoch:  9  Acc:  72.464376688  Change:  0.104600191116\n",
      "Epoch:  10  Acc:  72.4582195282  Change:  -0.0061571598053\n",
      "Epoch:  11  Acc:  72.4594533443  Change:  0.00123381614685\n",
      "Epoch:  12  Acc:  72.4779129028  Change:  0.0184595584869\n",
      "Epoch:  13  Acc:  72.4582195282  Change:  -0.0196933746338\n",
      "Epoch:  14  Acc:  72.5049853325  Change:  0.0467658042908\n",
      "Epoch:  15  Acc:  72.4939107895  Change:  -0.0110745429993\n",
      "Epoch:  16  Acc:  72.5012898445  Change:  0.00737905502319\n",
      "Epoch:  17  Acc:  72.4889874458  Change:  -0.0123023986816\n",
      "Epoch:  18  Acc:  72.4803745747  Change:  -0.00861287117004\n",
      "Epoch:  19  Acc:  72.4939107895  Change:  0.0135362148285\n",
      "Epoch:  20  Acc:  72.4483788013  Change:  -0.0455319881439\n",
      "Epoch:  21  Acc:  72.475451231  Change:  0.027072429657\n",
      "Epoch:  22  Acc:  72.4828362465  Change:  0.00738501548767\n",
      "Epoch:  23  Acc:  72.5492835045  Change:  0.0664472579956\n",
      "Epoch:  24  Acc:  72.5812792778  Change:  0.0319957733154\n",
      "Epoch:  25  Acc:  72.6255834103  Change:  0.0443041324615\n",
      "Epoch:  26  Acc:  72.6514220238  Change:  0.0258386135101\n",
      "Epoch:  27  Acc:  72.7301836014  Change:  0.0787615776062\n",
      "Epoch:  28  Acc:  72.7732539177  Change:  0.0430703163147\n",
      "Epoch:  29  Acc:  72.8138625622  Change:  0.0406086444855\n",
      "Epoch:  30  Acc:  72.8704690933  Change:  0.0566065311432\n",
      "Epoch:  31  Acc:  72.9295372963  Change:  0.0590682029724\n",
      "Epoch:  32  Acc:  72.9590713978  Change:  0.0295341014862\n",
      "Epoch:  33  Acc:  73.0132162571  Change:  0.054144859314\n",
      "Epoch:  34  Acc:  73.0415225029  Change:  0.0283062458038\n",
      "Epoch:  35  Acc:  73.1141269207  Change:  0.0726044178009\n",
      "Epoch:  36  Acc:  73.1941103935  Change:  0.0799834728241\n",
      "Epoch:  37  Acc:  73.2076466084  Change:  0.0135362148285\n",
      "Epoch:  38  Acc:  73.2433378696  Change:  0.0356912612915\n",
      "Epoch:  39  Acc:  73.2900977135  Change:  0.0467598438263\n",
      "Epoch:  40  Acc:  73.3651638031  Change:  0.0750660896301\n",
      "Epoch:  41  Acc:  73.3922362328  Change:  0.027072429657\n",
      "Epoch:  42  Acc:  73.4451532364  Change:  0.0529170036316\n",
      "Epoch:  43  Acc:  73.4759151936  Change:  0.0307619571686\n",
      "Epoch:  44  Acc:  73.5017597675  Change:  0.0258445739746\n",
      "Epoch:  45  Acc:  73.5263705254  Change:  0.0246107578278\n",
      "Epoch:  46  Acc:  73.5399067402  Change:  0.0135362148285\n",
      "Epoch:  47  Acc:  73.5559046268  Change:  0.0159978866577\n",
      "Epoch:  48  Acc:  73.5743641853  Change:  0.0184595584869\n",
      "Epoch:  49  Acc:  73.6100494862  Change:  0.035685300827\n",
      "Epoch:  50  Acc:  73.6445069313  Change:  0.0344574451447\n",
      "Epoch:  51  Acc:  73.6703515053  Change:  0.0258445739746\n",
      "Epoch:  52  Acc:  73.7195730209  Change:  0.0492215156555\n",
      "Epoch:  53  Acc:  73.7318813801  Change:  0.0123083591461\n",
      "Epoch:  54  Acc:  73.7564921379  Change:  0.0246107578278\n",
      "Epoch:  55  Acc:  73.7638771534  Change:  0.00738501548767\n",
      "Epoch:  56  Acc:  73.7786412239  Change:  0.0147640705109\n",
      "Epoch:  57  Acc:  74.0825951099  Change:  0.303953886032\n",
      "Epoch:  58  Acc:  74.1158246994  Change:  0.0332295894623\n",
      "Epoch:  59  Acc:  74.1207480431  Change:  0.00492334365845\n",
      "Epoch:  60  Acc:  74.1601228714  Change:  0.0393748283386\n",
      "Epoch:  61  Acc:  74.1810441017  Change:  0.0209212303162\n",
      "Epoch:  62  Acc:  74.213039875  Change:  0.0319957733154\n",
      "Epoch:  63  Acc:  74.2474973202  Change:  0.0344574451447\n",
      "Epoch:  64  Acc:  74.249958992  Change:  0.00246167182922\n",
      "Epoch:  65  Acc:  74.2757976055  Change:  0.0258386135101\n",
      "Epoch:  66  Acc:  74.2881059647  Change:  0.0123083591461\n",
      "Epoch:  67  Acc:  74.3077933788  Change:  0.0196874141693\n",
      "Epoch:  68  Acc:  74.3274867535  Change:  0.0196933746338\n",
      "Epoch:  69  Acc:  74.3484020233  Change:  0.0209152698517\n",
      "Epoch:  70  Acc:  74.3496358395  Change:  0.00123381614685\n",
      "Epoch:  71  Acc:  74.3680953979  Change:  0.0184595584869\n",
      "Epoch:  72  Acc:  74.375474453  Change:  0.00737905502319\n",
      "Epoch:  73  Acc:  74.3742465973  Change:  -0.00122785568237\n",
      "Epoch:  74  Acc:  74.3927061558  Change:  0.0184595584869\n",
      "Epoch:  75  Acc:  74.4062423706  Change:  0.0135362148285\n",
      "Epoch:  76  Acc:  74.4123935699  Change:  0.00615119934082\n",
      "Epoch:  77  Acc:  74.4247019291  Change:  0.0123083591461\n",
      "Epoch:  78  Acc:  74.4296252728  Change:  0.00492334365845\n",
      "Epoch:  79  Acc:  74.4542360306  Change:  0.0246107578278\n",
      "Epoch:  80  Acc:  74.4788467884  Change:  0.0246107578278\n",
      "Epoch:  81  Acc:  74.4911491871  Change:  0.0123023986816\n",
      "Epoch:  82  Acc:  74.5022296906  Change:  0.0110805034637\n",
      "Epoch:  83  Acc:  74.5133042336  Change:  0.0110745429993\n",
      "Epoch:  84  Acc:  74.5354533195  Change:  0.0221490859985\n",
      "Epoch:  85  Acc:  74.5465278625  Change:  0.0110745429993\n",
      "Epoch:  86  Acc:  74.5551407337  Change:  0.00861287117004\n",
      "Epoch:  87  Acc:  74.5711386204  Change:  0.0159978866577\n",
      "Epoch:  88  Acc:  74.5797514915  Change:  0.00861287117004\n",
      "Epoch:  89  Acc:  74.612981081  Change:  0.0332295894623\n",
      "Epoch:  90  Acc:  74.624055624  Change:  0.0110745429993\n",
      "Epoch:  91  Acc:  74.6302068233  Change:  0.00615119934082\n",
      "Epoch:  92  Acc:  74.6486663818  Change:  0.0184595584869\n",
      "Epoch:  93  Acc:  74.6622025967  Change:  0.0135362148285\n",
      "Epoch:  94  Acc:  74.6892750263  Change:  0.027072429657\n",
      "Epoch:  95  Acc:  74.7138857841  Change:  0.0246107578278\n",
      "Epoch:  96  Acc:  74.7298836708  Change:  0.0159978866577\n",
      "Epoch:  97  Acc:  74.7434198856  Change:  0.0135362148285\n",
      "Epoch:  98  Acc:  74.7581899166  Change:  0.0147700309753\n",
      "Epoch:  99  Acc:  74.7803390026  Change:  0.0221490859985\n",
      "Epoch:  100  Acc:  74.7840344906  Change:  0.00369548797607\n",
      "Epoch:  101  Acc:  74.7975707054  Change:  0.0135362148285\n",
      "Epoch:  102  Acc:  74.8135685921  Change:  0.0159978866577\n",
      "Epoch:  103  Acc:  74.8307943344  Change:  0.0172257423401\n",
      "Epoch:  104  Acc:  74.8357176781  Change:  0.00492334365845\n",
      "Epoch:  105  Acc:  74.8467922211  Change:  0.0110745429993\n",
      "Epoch:  106  Acc:  74.8554050922  Change:  0.00861287117004\n",
      "Epoch:  107  Acc:  74.8640179634  Change:  0.00861287117004\n",
      "Epoch:  108  Acc:  74.8849391937  Change:  0.0209212303162\n",
      "Epoch:  109  Acc:  74.8886346817  Change:  0.00369548797607\n",
      "Epoch:  110  Acc:  74.8997092247  Change:  0.0110745429993\n",
      "Epoch:  111  Acc:  74.9107837677  Change:  0.0110745429993\n",
      "Epoch:  112  Acc:  74.9157071114  Change:  0.00492334365845\n",
      "Epoch:  113  Acc:  74.9218583107  Change:  0.00615119934082\n",
      "Epoch:  114  Acc:  74.9378561974  Change:  0.0159978866577\n",
      "Epoch:  115  Acc:  74.942779541  Change:  0.00492334365845\n",
      "Epoch:  116  Acc:  74.9526202679  Change:  0.00984072685242\n",
      "Epoch:  117  Acc:  74.9636948109  Change:  0.0110745429993\n",
      "Epoch:  118  Acc:  74.9710798264  Change:  0.00738501548767\n",
      "Epoch:  119  Acc:  74.9883115292  Change:  0.0172317028046\n",
      "Epoch:  120  Acc:  75.0030755997  Change:  0.0147640705109\n",
      "Epoch:  121  Acc:  75.0116884708  Change:  0.00861287117004\n",
      "Epoch:  122  Acc:  75.0227630138  Change:  0.0110745429993\n",
      "Epoch:  123  Acc:  75.0436842442  Change:  0.0209212303162\n",
      "Epoch:  124  Acc:  75.0535309315  Change:  0.00984668731689\n",
      "Epoch:  125  Acc:  75.0633776188  Change:  0.00984668731689\n",
      "Epoch:  126  Acc:  75.0658392906  Change:  0.00246167182922\n",
      "Epoch:  127  Acc:  75.0793755054  Change:  0.0135362148285\n",
      "Epoch:  128  Acc:  75.1015245914  Change:  0.0221490859985\n",
      "Epoch:  129  Acc:  75.1101374626  Change:  0.00861287117004\n",
      "Epoch:  130  Acc:  75.1125991344  Change:  0.00246167182922\n",
      "Epoch:  131  Acc:  75.1138269901  Change:  0.00122785568237\n",
      "Epoch:  132  Acc:  75.1113653183  Change:  -0.00246167182922\n",
      "Epoch:  133  Acc:  75.1175224781  Change:  0.0061571598053\n",
      "Epoch:  134  Acc:  75.1187503338  Change:  0.00122785568237\n",
      "Epoch:  135  Acc:  75.1249074936  Change:  0.0061571598053\n",
      "Epoch:  136  Acc:  75.1396715641  Change:  0.0147640705109\n",
      "Epoch:  137  Acc:  75.1384437084  Change:  -0.00122785568237\n",
      "Epoch:  138  Acc:  75.1421332359  Change:  0.0036895275116\n",
      "Epoch:  139  Acc:  75.1556694508  Change:  0.0135362148285\n",
      "Epoch:  140  Acc:  75.1642823219  Change:  0.00861287117004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  141  Acc:  75.1728951931  Change:  0.00861287117004\n",
      "Epoch:  142  Acc:  75.1876652241  Change:  0.0147700309753\n",
      "Epoch:  143  Acc:  75.1962780952  Change:  0.00861287117004\n",
      "Epoch:  144  Acc:  75.1962780952  Change:  0.0\n",
      "Epoch:  145  Acc:  75.1987397671  Change:  0.00246167182922\n",
      "Epoch:  146  Acc:  75.1999676228  Change:  0.00122785568237\n",
      "Epoch:  147  Acc:  75.2122759819  Change:  0.0123083591461\n",
      "Epoch:  148  Acc:  75.2208888531  Change:  0.00861287117004\n",
      "Epoch:  149  Acc:  75.2295017242  Change:  0.00861287117004\n",
      "Epoch:  150  Acc:  75.2393484116  Change:  0.00984668731689\n",
      "Epoch:  151  Acc:  75.2454996109  Change:  0.00615119934082\n",
      "Epoch:  152  Acc:  75.25780797  Change:  0.0123083591461\n",
      "Epoch:  153  Acc:  75.272578001  Change:  0.0147700309753\n",
      "Epoch:  154  Acc:  75.272578001  Change:  0.0\n",
      "Epoch:  155  Acc:  75.2738058567  Change:  0.00122785568237\n",
      "Epoch:  156  Acc:  75.2824187279  Change:  0.00861287117004\n",
      "Epoch:  157  Acc:  75.2848803997  Change:  0.00246167182922\n",
      "Epoch:  158  Acc:  75.294727087  Change:  0.00984668731689\n",
      "Epoch:  159  Acc:  75.2922654152  Change:  -0.00246167182922\n",
      "Epoch:  160  Acc:  75.3008782864  Change:  0.00861287117004\n",
      "Epoch:  161  Acc:  75.3008782864  Change:  0.0\n",
      "Epoch:  162  Acc:  75.3119528294  Change:  0.0110745429993\n",
      "Epoch:  163  Acc:  75.3119528294  Change:  0.0\n",
      "Epoch:  164  Acc:  75.3230273724  Change:  0.0110745429993\n",
      "Epoch:  165  Acc:  75.3353357315  Change:  0.0123083591461\n",
      "Epoch:  166  Acc:  75.3365635872  Change:  0.00122785568237\n",
      "Epoch:  167  Acc:  75.3451824188  Change:  0.00861883163452\n",
      "Epoch:  168  Acc:  75.3488719463  Change:  0.0036895275116\n",
      "Epoch:  169  Acc:  75.3574848175  Change:  0.00861287117004\n",
      "Epoch:  170  Acc:  75.3673315048  Change:  0.00984668731689\n",
      "Epoch:  171  Acc:  75.3636360168  Change:  -0.00369548797607\n",
      "Epoch:  172  Acc:  75.3784060478  Change:  0.0147700309753\n",
      "Epoch:  173  Acc:  75.3796339035  Change:  0.00122785568237\n",
      "Epoch:  174  Acc:  75.3845572472  Change:  0.00492334365845\n",
      "Epoch:  175  Acc:  75.3894805908  Change:  0.00492334365845\n",
      "Epoch:  176  Acc:  75.387018919  Change:  -0.00246167182922\n",
      "Epoch:  177  Acc:  75.3919422626  Change:  0.00492334365845\n",
      "Epoch:  178  Acc:  75.40178895  Change:  0.00984668731689\n",
      "Epoch:  179  Acc:  75.4165530205  Change:  0.0147640705109\n",
      "Epoch:  180  Acc:  75.4177868366  Change:  0.00123381614685\n",
      "Epoch:  181  Acc:  75.4325509071  Change:  0.0147640705109\n",
      "Epoch:  182  Acc:  75.4387021065  Change:  0.00615119934082\n",
      "Epoch:  183  Acc:  75.4448592663  Change:  0.0061571598053\n",
      "Epoch:  184  Acc:  75.457161665  Change:  0.0123023986816\n",
      "Epoch:  185  Acc:  75.468236208  Change:  0.0110745429993\n",
      "Epoch:  186  Acc:  75.4743933678  Change:  0.0061571598053\n",
      "Epoch:  187  Acc:  75.4928529263  Change:  0.0184595584869\n",
      "Epoch:  188  Acc:  75.4965424538  Change:  0.0036895275116\n",
      "Epoch:  189  Acc:  75.5039274693  Change:  0.00738501548767\n",
      "Epoch:  190  Acc:  75.5236148834  Change:  0.0196874141693\n",
      "Epoch:  191  Acc:  75.5420744419  Change:  0.0184595584869\n",
      "Epoch:  192  Acc:  75.5433022976  Change:  0.00122785568237\n",
      "Epoch:  193  Acc:  75.5556106567  Change:  0.0123083591461\n",
      "Epoch:  194  Acc:  75.5629956722  Change:  0.00738501548767\n",
      "Epoch:  195  Acc:  75.5839109421  Change:  0.0209152698517\n",
      "Epoch:  196  Acc:  75.5839109421  Change:  0.0\n",
      "Epoch:  197  Acc:  75.5925297737  Change:  0.00861883163452\n",
      "Epoch:  198  Acc:  75.5962193012  Change:  0.0036895275116\n",
      "Epoch:  199  Acc:  75.5999088287  Change:  0.0036895275116\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "learning_rate = 0.10\n",
    "\n",
    "activation_func = tf.tanh\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Placeholders for the actual data.\n",
    "    train_features = tf.placeholder(tf.float32, shape=(None, 480))\n",
    "    train_labels = tf.placeholder(tf.int32, shape=(None, ))\n",
    "    \n",
    "    # Reshaping the data\n",
    "    x = tf.reshape(train_features, [-1, 160, 3])\n",
    "    y = tf.one_hot(\n",
    "        train_labels,\n",
    "        depth=K\n",
    "    )\n",
    "    \n",
    "    # Convolution 1\n",
    "    conv1 = tf.layers.conv1d(inputs=x, \n",
    "                             filters=8, \n",
    "                             kernel_size=8, \n",
    "                             padding=\"same\", \n",
    "                             data_format=\"channels_last\",\n",
    "                             activation=activation_func)\n",
    "    # Max Pooling 1 (reduces samples from 80 --> 31)\n",
    "    pool1 = tf.layers.max_pooling1d(inputs=conv1,\n",
    "                                    pool_size=80, \n",
    "                                    strides=10,\n",
    "                                    data_format=\"channels_last\")\n",
    "    \n",
    "    # Convolution 2\n",
    "    conv2 = tf.layers.conv1d(inputs=pool1, \n",
    "                             filters=16, \n",
    "                             kernel_size=8, \n",
    "                             padding=\"same\", \n",
    "                             data_format=\"channels_last\",\n",
    "                             activation=activation_func)\n",
    "    # Max Pooling 2 (reduces samples from 39 --> 17)\n",
    "    pool2 = tf.layers.max_pooling1d(inputs=conv2,\n",
    "                                    pool_size=3, \n",
    "                                    strides=3,\n",
    "                                    data_format=\"channels_last\")\n",
    "    \n",
    "    # Flatten the Pooled Data\n",
    "    pool2_flat = tf.reshape(pool2, [-1, 3 * 16])\n",
    "    \n",
    "    # Dense Layer (try adding dropout?)\n",
    "    dense = tf.layers.dense(inputs=pool2_flat, units=128, activation=activation_func)\n",
    "    \n",
    "    # Logits Layer\n",
    "    logits = tf.layers.dense(inputs=dense, units=K)\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=logits, \n",
    "            labels=y\n",
    "        )\n",
    "    )\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    # Define accuracy (so that it can be calculated and printed)\n",
    "    accuracy = tf.reduce_mean(\n",
    "        tf.cast(\n",
    "            tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1)),\n",
    "            tf.float32\n",
    "        )\n",
    "    )\n",
    "    \n",
    "with tf.Session(graph=graph) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    i = 0\n",
    "    prev = 0\n",
    "    for _ in range(num_epochs):\n",
    "        feed_dict = {\n",
    "            train_features: features,\n",
    "            train_labels: labels\n",
    "        }\n",
    "        _, _, acc = sess.run(\n",
    "            [optimizer, loss, accuracy], \n",
    "            feed_dict=feed_dict)\n",
    "        print(\"Epoch: \", i, \" Acc: \", acc*100, \" Change: \", (acc-prev)*100)\n",
    "        i = i + 1\n",
    "        prev = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An attempt at Dennis Sun's way of convolving the first second and the second second of data independently then combining\n",
    "The code below does not work yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## defining needed variables\n",
    "labels = train['activity']\n",
    "K = len(train['activity'].unique())\n",
    "# features = train[np.arange(480).astype(str)]\n",
    "features1 = train[np.arange(240).astype(str)]\n",
    "features2 = train[np.arange(240,480).astype(str)]\n",
    "\n",
    "num_epochs = 200\n",
    "learning_rate = 0.10\n",
    "\n",
    "activation_func = tf.tanh\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Placeholders for the actual data.\n",
    "    train_features = tf.placeholder(tf.float32, shape=(None, 240))\n",
    "    train_labels = tf.placeholder(tf.int32, shape=(None, ))\n",
    "    \n",
    "    # Reshaping the data\n",
    "    x = tf.reshape(train_features, [-1, 80, 3])\n",
    "    y = tf.one_hot(\n",
    "        train_labels,\n",
    "        depth=K\n",
    "    )\n",
    "    \n",
    "    # Convolution 1\n",
    "    conv1 = tf.layers.conv1d(inputs=x, \n",
    "                             filters=8, \n",
    "                             kernel_size=8, \n",
    "                             padding=\"same\", \n",
    "                             data_format=\"channels_last\",\n",
    "                             activation=activation_func)\n",
    "    # Max Pooling 1 (reduces samples from 80 --> 31)\n",
    "    pool1 = tf.layers.max_pooling1d(inputs=conv1,\n",
    "                                    pool_size=30, \n",
    "                                    strides=5,\n",
    "                                    data_format=\"channels_last\")\n",
    "    \n",
    "    # Convolution 2\n",
    "    conv2 = tf.layers.conv1d(inputs=pool1, \n",
    "                             filters=16, \n",
    "                             kernel_size=8, \n",
    "                             padding=\"same\", \n",
    "                             data_format=\"channels_last\",\n",
    "                             activation=activation_func)\n",
    "    # Max Pooling 2 (reduces samples from 39 --> 17)\n",
    "    pool2 = tf.layers.max_pooling1d(inputs=conv2,\n",
    "                                    pool_size=2, \n",
    "                                    strides=1,\n",
    "                                    data_format=\"channels_last\")\n",
    "    \n",
    "    # Flatten the Pooled Data\n",
    "    pool2_flat = tf.reshape(pool2, [-1, 3 * 16])\n",
    "    \n",
    "    # Dense Layer (try adding dropout?)\n",
    "    dense = tf.layers.dense(inputs=pool2_flat, units=128, activation=activation_func)\n",
    "    \n",
    "    # Logits Layer\n",
    "    logits = tf.layers.dense(inputs=dense, units=K)\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=logits, \n",
    "            labels=y\n",
    "        )\n",
    "    )\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    # Define accuracy (so that it can be calculated and printed)\n",
    "    accuracy = tf.reduce_mean(\n",
    "        tf.cast(\n",
    "            tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1)),\n",
    "            tf.float32\n",
    "        )\n",
    "    )\n",
    "    \n",
    "with tf.Session(graph=graph) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    i = 0\n",
    "    prev = 0\n",
    "    for _ in range(num_epochs):\n",
    "        feed_dict = {\n",
    "            train_features: features,\n",
    "            train_labels: labels\n",
    "        }\n",
    "        _, _, acc = sess.run(\n",
    "            [optimizer, loss, accuracy], \n",
    "            feed_dict=feed_dict)\n",
    "        print(\"Epoch: \", i, \" Acc: \", acc*100, \" Change: \", (acc-prev)*100)\n",
    "        i = i + 1\n",
    "        prev = acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
